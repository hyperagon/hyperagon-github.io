+++
title = 'Ollama Shenanigans'
summary = 'How I got to use Ollama in the Shell'
date = 2024-03-14T08:40:11Z
draft = false
tags = ['ollama']
+++
[Ollama](https://ollama.com/) is a tool that allows one to run *LLMs* locally. Unfortunately I read some outdated? guides (probably targetting a different Operating System) and tried to use tools designed to work with [ChatGPT](https://chat.openai.com/auth/login). This, of course involves paying for it and was outside of my objective.

Looking at the [model page](https://ollama.com/library/deepseek-coder) finally got me the correct address point: `http://localhost:11434/api/generate` but it didn't work with [GPT Pilot](https://github.com/Pythagora-io/gpt-pilot) so I used a [python package](https://github.com/ollama/ollama-python) and just copied the prompts.

So I made a script around it:
```
#!/usr/bin/env python
import sys
import signal
import ollama

USER = 'user'
MODEL = 'deepseek-coder'

def signal_handler(sig, frame):
    print('\nYou pressed Ctrl+C!')
    sys.exit(0)

def prompt(prompt, context=''):
    try:
        return ollama.chat(model=MODEL, messages=[
            {
            'role': USER,
            'content': prompt,
            },
        ])
    except ollama.ResponseError as e:
      print('Error:', e.error)
      if e.status_code == 404:
        ollama.pull(model)
        return []

try:
    query = ''
    signal.signal(signal.SIGINT, signal_handler)
    while True:
        query = input('> ')
        if  query.lower() == 'exit' or query.lower() == 'quit':
            break
        if len(query) > 0:
            print(prompt(query)['message']['content'])
except Exception as ex:
    print(ex)
```

Now I can have it read files and test things.
